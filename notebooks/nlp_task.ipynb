{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Su-sid/Eskalate-ML-DS-Interview-NLP-for-Information-Extraction-Summarization-Agentic-AI-Design/blob/main/notebooks/nlp_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl_iMRV-lbuX"
      },
      "source": [
        "# Nlp Intelligence System Interview - David Sudi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eTzQL0BzyeyJ",
        "outputId": "628a6ec7-6e37-4d3a-d7e4-02ded0d5d618"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q nltk spacy pandas matplotlib seaborn groq transformers scikit-learn kagglehub agno rouge-score\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZEZy9GxlHuU",
        "outputId": "985ad27e-7cff-4c94-e368-b837f88d7452"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from typing import List, Dict, Any, Optional\n",
        "import kagglehub\n",
        "import random\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "# Agno imports\n",
        "from agno.agent import Agent\n",
        "from agno.models.openai import OpenAIChat\n",
        "from agno.models.groq import Groq\n",
        "from agno.tools import tool\n",
        "from agno.utils.pprint import pprint_run_response\n",
        "\n",
        "# ROUGE for evaluation\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "    ROUGE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ROUGE_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è ROUGE not available, will use alternative metrics\")\n",
        "\n",
        "# NLTK downloads\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('vader_lexicon')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"‚úÖ All imports and setup completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPLQ8tZYj89N"
      },
      "outputs": [],
      "source": [
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "# os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "\n",
        "print(api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OclgU21d7OIc",
        "outputId": "008ef0b7-634a-4705-e32c-33a4f2dad2ac"
      },
      "outputs": [],
      "source": [
        "# load nltk data and spacy model\n",
        "\n",
        "#  NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#  spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(\"data and model loaded \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfGeUfc1Hn5"
      },
      "source": [
        "DATA LOADING AND EXPOLORATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VPBa0zzydFo",
        "outputId": "b0802a97-d61d-42dc-b402-bcd1ca3b1b29"
      },
      "outputs": [],
      "source": [
        "# load dataset from kaggle\n",
        "\n",
        "print(\" Downloading News Dataset...\")\n",
        "path = kagglehub.dataset_download(\"rmisra/news-category-dataset\")\n",
        "print(f\"Path to dataset files: {path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXO0mjf01lPA",
        "outputId": "8e22beaf-d72e-4022-cc4b-bdbe70336e07"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "data_files = [f for f in os.listdir(path) if f.endswith('.json')]\n",
        "print(f\"Found files: {data_files}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2Vaze7T1o38",
        "outputId": "9eeca19d-e5d9-411b-b2b3-a381a0744315"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the JSON data\n",
        "df_list = []\n",
        "for file in data_files:\n",
        "    file_path = os.path.join(path, file)\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            df_list.append(json.loads(line))\n",
        "\n",
        "dfMain = pd.DataFrame(df_list)\n",
        "\n",
        "df= dfMain.copy()\n",
        "print(f\" Dataset loaded: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZjfMxp31rVI",
        "outputId": "08764122-eedf-499f-ed24-15a12be246c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Display basic info\n",
        "print(\"\\nüìà Dataset Overview:\")\n",
        "print(df.info())\n",
        "print(f\"\\nSample record:\")\n",
        "print(df.head(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Kfsx-F1tXc",
        "outputId": "d16047f6-230e-462c-dd66-d8ca69efb6b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Select subset for faster processing (adjust size based on your needs)\n",
        "SAMPLE_SIZE = 5000\n",
        "df_sample = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).reset_index(drop=True)\n",
        "print(f\"\\n Working with sample of {len(df_sample)} articles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3JXGN5q4Up4"
      },
      "source": [
        "# PART 3: EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "jwFFRYyY4MZx",
        "outputId": "f1a6a443-ec76-4956-dbb8-4cd89bb15160"
      },
      "outputs": [],
      "source": [
        "\n",
        "def perform_eda(df):\n",
        "    \"\"\"Perform exploratory data analysis\"\"\"\n",
        "    print(\"üîç EXPLORATORY DATA ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Add combined text and length\n",
        "    df['combined_text'] = df['headline'].fillna('') + ' ' + df['short_description'].fillna('')\n",
        "    df['text_length'] = df['combined_text'].str.len()\n",
        "\n",
        "    # Category distribution\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    category_counts = df['category'].value_counts().head(10)\n",
        "    sns.barplot(x=category_counts.values, y=category_counts.index)\n",
        "    plt.title('Top 10 News Categories')\n",
        "    plt.xlabel('Count')\n",
        "\n",
        "    # Document length distribution\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.hist(df['text_length'], bins=30, alpha=0.7, color='skyblue')\n",
        "    plt.title('Document Length Distribution')\n",
        "    plt.xlabel('Character Count')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Words per article\n",
        "    df['word_count'] = df['combined_text'].str.split().str.len()\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.hist(df['word_count'], bins=30, alpha=0.7, color='lightgreen')\n",
        "    plt.title('Words per Article Distribution')\n",
        "    plt.xlabel('Word Count')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Timeline analysis (if date available)\n",
        "    plt.subplot(2, 2, 4)\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df['year'] = df['date'].dt.year\n",
        "        year_counts = df['year'].value_counts().sort_index()\n",
        "        plt.plot(year_counts.index, year_counts.values, marker='o')\n",
        "        plt.title('Articles by Year')\n",
        "        plt.xlabel('Year')\n",
        "        plt.ylabel('Count')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Date analysis not available', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title('Date Analysis')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\nüìä DATASET STATISTICS:\")\n",
        "    print(f\"Average text length: {df['text_length'].mean():.2f} characters\")\n",
        "    print(f\"Average word count: {df['word_count'].mean():.2f} words\")\n",
        "    print(f\"Categories: {df['category'].nunique()}\")\n",
        "    print(f\"Most common category: {df['category'].mode()[0]}\")\n",
        "    if 'date' in df.columns:\n",
        "        print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Run EDA\n",
        "df_sample = perform_eda(df_sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXutALSA6jw5"
      },
      "source": [
        "# PART 4: TEXT PREPROCESSING TOOLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoaGiSd56cU7",
        "outputId": "f7bc5875-fbd9-4060-c982-fe9a746e08dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Advanced text preprocessing utility\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Remove special characters and extra whitespace\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_and_filter(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize and remove stop words\"\"\"\n",
        "        tokens = nltk.word_tokenize(self.clean_text(text))\n",
        "        return [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Add processed text columns\n",
        "print(\"üîÑ Preprocessing text...\")\n",
        "df_sample['processed_text'] = df_sample['combined_text'].apply(preprocessor.clean_text)\n",
        "df_sample['tokens'] = df_sample['combined_text'].apply(preprocessor.tokenize_and_filter)\n",
        "\n",
        "# Display preprocessing results\n",
        "print(\"\\nüìã Preprocessing Sample:\")\n",
        "sample_idx = 0\n",
        "print(f\"Original: {df_sample['combined_text'].iloc[sample_idx][:100]}...\")\n",
        "print(f\"Processed: {df_sample['processed_text'].iloc[sample_idx][:100]}...\")\n",
        "print(f\"Tokens (first 10): {df_sample['tokens'].iloc[sample_idx][:10]}\")\n",
        "\n",
        "print(\"‚úÖ Text preprocessing completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnq7CwB9NaHH",
        "outputId": "12a44de5-856c-410f-91e7-54d349912fde"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 6: INFORMATION EXTRACTION TOOLS\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def extract_entities_ner_func(text: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Extract named entities from text using spaCy NER.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[str]]: Dictionary with entity types as keys and entity lists as values\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    entities = defaultdict(list)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.label_].append(ent.text)\n",
        "\n",
        "    # Convert to regular dict and remove duplicates\n",
        "    result = {}\n",
        "    for label, ents in entities.items():\n",
        "        result[label] = list(set(ents))\n",
        "\n",
        "    return result\n",
        "@tool(\n",
        "    name=\"extract_named_entities\",\n",
        "    description=\"Extract named entities (persons, organizations, locations) from text using spaCy NER\",\n",
        "    show_result=False,\n",
        "    cache_results=True,\n",
        "    cache_ttl=3600\n",
        ")\n",
        "def extract_entities_ner(text: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"Tool version that calls the original function\"\"\"\n",
        "    return extract_entities_ner_func(text)\n",
        "\n",
        "\n",
        "@tool(\n",
        "    name=\"extract_patterns_regex\",\n",
        "    description=\"Extract structured patterns (dates, URLs, emails, numbers) from text using regex\",\n",
        "    show_result=False,\n",
        "    cache_results=True\n",
        ")\n",
        "def extract_entities_regex_func(text: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Extract entities using regex patterns.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[str]]: Dictionary with extracted patterns\n",
        "    \"\"\"\n",
        "    patterns = {\n",
        "        'dates': r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}\\b',\n",
        "        'urls': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
        "        'emails': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "        'phone_numbers': r'\\b(?:\\+?1[-.\\s]?)?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\\b',\n",
        "        'currencies': r'\\$\\d+(?:\\.\\d{2})?|\\b\\d+(?:\\.\\d{2})?\\s*(?:dollars?|USD|euros?|EUR)\\b',\n",
        "        'percentages': r'\\b\\d+(?:\\.\\d+)?%\\b'\n",
        "    }\n",
        "\n",
        "    extracted = {}\n",
        "    for pattern_name, pattern in patterns.items():\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        extracted[pattern_name] = list(set(matches)) if matches else []\n",
        "\n",
        "    return extracted\n",
        "\n",
        "# Tool version for the agent\n",
        "@tool(\n",
        "    name=\"extract_patterns_regex\",\n",
        "    description=\"Extract structured patterns (dates, URLs, emails, numbers) from text using regex\",\n",
        "    show_result=False,\n",
        "    cache_results=True\n",
        ")\n",
        "def extract_entities_regex(text: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"Tool version that calls the original function\"\"\"\n",
        "    return extract_entities_regex_func(text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_top_keywords_func(text: str, n: int = 10) -> List[tuple]:\n",
        "    \"\"\"\n",
        "    Extract top keywords using TF-IDF.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "        n (int): Number of top keywords to return\n",
        "\n",
        "    Returns:\n",
        "        List[tuple]: List of (keyword, score) tuples\n",
        "    \"\"\"\n",
        "    # Preprocess text\n",
        "    tokens = preprocessor.tokenize_and_filter(text)\n",
        "\n",
        "    if not tokens:\n",
        "        return []\n",
        "\n",
        "    # Simple TF calculation\n",
        "    word_freq = Counter(tokens)\n",
        "    total_words = len(tokens)\n",
        "\n",
        "    # Calculate TF scores\n",
        "    tf_scores = [(word, count/total_words) for word, count in word_freq.most_common(n)]\n",
        "\n",
        "    return tf_scores\n",
        "\n",
        "@tool(\n",
        "    name=\"extract_keywords_tfidf\",\n",
        "    description=\"Extract top keywords from text using TF-IDF scoring\",\n",
        "    show_result=False\n",
        ")\n",
        "def get_top_keywords(text: str, n: int = 10) -> List[tuple]:\n",
        "    return get_top_keywords_func(text, n = 10)\n",
        "\n",
        "print(\"‚úÖ Information extraction tools created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BBtPsUzNc6p",
        "outputId": "92afd0b0-377c-4bec-b839-083556b0adf4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 7: SUMMARIZATION TOOLS\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def extractive_summarize_func(text: str, num_sentences: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Generate extractive summary using TF-IDF sentence ranking.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to summarize\n",
        "        num_sentences (int): Number of sentences in summary\n",
        "\n",
        "    Returns:\n",
        "        str: Extractive summary\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return text\n",
        "\n",
        "    # Preprocess sentences\n",
        "    processed_sentences = [preprocessor.clean_text(sent) for sent in sentences]\n",
        "\n",
        "    try:\n",
        "        # Create TF-IDF matrix\n",
        "        vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "        tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
        "\n",
        "        # Calculate sentence scores (sum of TF-IDF values)\n",
        "        sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
        "\n",
        "        # Get top sentences\n",
        "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
        "        top_indices = sorted(top_indices)  # Maintain original order\n",
        "\n",
        "        summary_sentences = [sentences[i] for i in top_indices]\n",
        "        return ' '.join(summary_sentences)\n",
        "    except:\n",
        "        # Fallback: return first few sentences\n",
        "        return ' '.join(sentences[:num_sentences])\n",
        "\n",
        "@tool(\n",
        "    name=\"extractive_summarization\",\n",
        "    description=\"Generate extractive summary using TF-IDF sentence ranking\",\n",
        "    show_result=False,\n",
        "    cache_results=True\n",
        ")\n",
        "def extractive_summarize(text: str, num_sentences: int = 3) -> str:\n",
        "    \"\"\"Tool version\"\"\"\n",
        "    return extractive_summarize_func(text, num_sentences)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def simple_abstractive_summarize_func(text: str, max_words: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    Generate a simple abstractive summary by combining key sentences and phrases.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to summarize\n",
        "        max_words (int): Maximum words in summary\n",
        "\n",
        "    Returns:\n",
        "        str: Abstractive-style summary\n",
        "    \"\"\"\n",
        "    # Get key sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    if len(sentences) <= 2:\n",
        "        return text\n",
        "\n",
        "    # Get keywords\n",
        "    keywords = get_top_keywords(text, 8)\n",
        "    key_terms = [kw[0] for kw in keywords]\n",
        "\n",
        "    # Find sentences with most keywords\n",
        "    sentence_scores = []\n",
        "    for sent in sentences:\n",
        "        sent_lower = sent.lower()\n",
        "        score = sum(1 for term in key_terms if term in sent_lower)\n",
        "        sentence_scores.append((sent, score))\n",
        "\n",
        "    # Sort by score and select top sentences\n",
        "    sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Build summary within word limit\n",
        "    summary_parts = []\n",
        "    word_count = 0\n",
        "\n",
        "    for sent, score in sentence_scores:\n",
        "        sent_words = len(sent.split())\n",
        "        if word_count + sent_words <= max_words:\n",
        "            summary_parts.append(sent)\n",
        "            word_count += sent_words\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if not summary_parts:\n",
        "        summary_parts = [sentences[0]]  # Fallback\n",
        "\n",
        "    return ' '.join(summary_parts)\n",
        "\n",
        "@tool(\n",
        "    name=\"abstractive_summarization\",\n",
        "    description=\"Generate abstractive summary using language model\",\n",
        "    show_result=False,\n",
        "    requires_confirmation=False\n",
        ")\n",
        "def simple_abstractive_summarize(text: str, max_words: int = 50) -> str:\n",
        "    \"\"\"Tool version\"\"\"\n",
        "    return simple_abstractive_summarize_func(text, max_words)\n",
        "\n",
        "print(\"‚úÖ Summarization tools created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVHa-_-ENwPt",
        "outputId": "5a06667d-99db-4726-a0bf-54944ec28e3c"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 8: EVALUATION SYSTEM\n",
        "# =============================================================================\n",
        "\n",
        "class ComprehensiveEvaluator:\n",
        "    \"\"\"Advanced evaluation system for NLP tasks\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        if ROUGE_AVAILABLE:\n",
        "            self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    def evaluate_entity_extraction(self, text: str, entities_ner: Dict, entities_regex: Dict) -> Dict:\n",
        "        \"\"\"Comprehensive entity extraction evaluation with qualitative examples\"\"\"\n",
        "\n",
        "        evaluation = {\n",
        "            'text_preview': text[:200] + \"...\" if len(text) > 200 else text,\n",
        "            'ner_analysis': self._analyze_entities(entities_ner, \"Named Entity Recognition\"),\n",
        "            'regex_analysis': self._analyze_entities(entities_regex, \"Regex Pattern Extraction\"),\n",
        "            'quality_assessment': {},\n",
        "            'examples': {}\n",
        "        }\n",
        "\n",
        "        # Quality indicators\n",
        "        total_ner = sum(len(ents) for ents in entities_ner.values())\n",
        "        total_regex = sum(len(ents) for ents in entities_regex.values())\n",
        "\n",
        "        evaluation['quality_assessment'] = {\n",
        "            'total_entities_ner': total_ner,\n",
        "            'total_entities_regex': total_regex,\n",
        "            'entity_coverage': 'High' if (total_ner + total_regex) >= 5 else 'Medium' if (total_ner + total_regex) >= 2 else 'Low',\n",
        "            'diversity_score': len(entities_ner.keys()) + len(entities_regex.keys()),\n",
        "            'extraction_success': total_ner > 0 or total_regex > 0\n",
        "        }\n",
        "\n",
        "        # Qualitative examples\n",
        "        evaluation['examples']['ner_samples'] = self._get_entity_samples(entities_ner)\n",
        "        evaluation['examples']['regex_samples'] = self._get_entity_samples(entities_regex)\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    def evaluate_summarization(self, original_text: str, extractive_summary: str,\n",
        "                             abstractive_summary: str, reference_summary: str = None) -> Dict:\n",
        "        \"\"\"Comprehensive summarization evaluation with qualitative analysis\"\"\"\n",
        "\n",
        "        evaluation = {\n",
        "            'text_info': {\n",
        "                'original_length_words': len(original_text.split()),\n",
        "                'original_length_chars': len(original_text),\n",
        "                'original_sentences': len(nltk.sent_tokenize(original_text))\n",
        "            },\n",
        "            'extractive_evaluation': self._evaluate_single_summary(original_text, extractive_summary, \"Extractive\"),\n",
        "            'abstractive_evaluation': self._evaluate_single_summary(original_text, abstractive_summary, \"Abstractive\"),\n",
        "            'comparative_analysis': {},\n",
        "            'qualitative_examples': {}\n",
        "        }\n",
        "\n",
        "        # Comparative analysis\n",
        "        ext_score = evaluation['extractive_evaluation']['content_preservation']['similarity_score']\n",
        "        abs_score = evaluation['abstractive_evaluation']['content_preservation']['similarity_score']\n",
        "\n",
        "        evaluation['comparative_analysis'] = {\n",
        "            'better_content_preservation': 'Extractive' if ext_score > abs_score else 'Abstractive',\n",
        "            'extractive_compression': evaluation['extractive_evaluation']['compression']['ratio'],\n",
        "            'abstractive_compression': evaluation['abstractive_evaluation']['compression']['ratio'],\n",
        "            'coherence_comparison': {\n",
        "                'extractive': evaluation['extractive_evaluation']['coherence']['coherence_score'],\n",
        "                'abstractive': evaluation['abstractive_evaluation']['coherence']['coherence_score']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Qualitative examples\n",
        "        evaluation['qualitative_examples'] = {\n",
        "            'original_excerpt': original_text[:300] + \"...\" if len(original_text) > 300 else original_text,\n",
        "            'extractive_summary': extractive_summary,\n",
        "            'abstractive_summary': abstractive_summary,\n",
        "            'summary_comparison': self._compare_summaries(extractive_summary, abstractive_summary)\n",
        "        }\n",
        "\n",
        "        # ROUGE evaluation if reference available\n",
        "        if reference_summary and ROUGE_AVAILABLE:\n",
        "            evaluation['rouge_scores'] = self._calculate_rouge_scores(\n",
        "                extractive_summary, abstractive_summary, reference_summary\n",
        "            )\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    def _analyze_entities(self, entities: Dict, method_name: str) -> Dict:\n",
        "        \"\"\"Analyze entity extraction results\"\"\"\n",
        "        analysis = {\n",
        "            'method': method_name,\n",
        "            'entity_types_found': len(entities),\n",
        "            'total_entities': sum(len(ents) for ents in entities.values()),\n",
        "            'entity_breakdown': {}\n",
        "        }\n",
        "\n",
        "        for entity_type, entity_list in entities.items():\n",
        "            if entity_list:\n",
        "                analysis['entity_breakdown'][entity_type] = {\n",
        "                    'count': len(entity_list),\n",
        "                    'unique_count': len(set(entity_list)),\n",
        "                    'samples': entity_list[:3]  # First 3 examples\n",
        "                }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _get_entity_samples(self, entities: Dict) -> Dict:\n",
        "        \"\"\"Get sample entities for qualitative review\"\"\"\n",
        "        samples = {}\n",
        "        for entity_type, entity_list in entities.items():\n",
        "            if entity_list:\n",
        "                samples[entity_type] = entity_list[:3]  # Top 3 examples\n",
        "        return samples\n",
        "\n",
        "    def _evaluate_single_summary(self, original: str, summary: str, method: str) -> Dict:\n",
        "        \"\"\"Evaluate a single summary comprehensively\"\"\"\n",
        "        orig_words = len(original.split())\n",
        "        summ_words = len(summary.split())\n",
        "\n",
        "        evaluation = {\n",
        "            'method': method,\n",
        "            'compression': {\n",
        "                'ratio': summ_words / orig_words if orig_words > 0 else 0,\n",
        "                'original_words': orig_words,\n",
        "                'summary_words': summ_words,\n",
        "                'compression_percentage': (1 - summ_words / orig_words) * 100 if orig_words > 0 else 0\n",
        "            },\n",
        "            'coherence': self._assess_coherence(summary),\n",
        "            'content_preservation': self._assess_content_preservation(original, summary)\n",
        "        }\n",
        "\n",
        "        return evaluation\n",
        "\n",
        "    def _assess_coherence(self, text: str) -> Dict:\n",
        "        \"\"\"Assess text coherence using linguistic indicators\"\"\"\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        if not sentences:\n",
        "            return {'coherence_score': 0, 'sentence_count': 0, 'avg_sentence_length': 0}\n",
        "\n",
        "        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)\n",
        "\n",
        "        # Check for discourse markers\n",
        "        discourse_markers = ['however', 'therefore', 'furthermore', 'moreover', 'additionally',\n",
        "                           'consequently', 'meanwhile', 'similarly', 'in contrast', 'as a result',\n",
        "                           'first', 'second', 'finally', 'also', 'furthermore']\n",
        "\n",
        "        marker_count = sum(1 for marker in discourse_markers if marker in text.lower())\n",
        "\n",
        "        # Simple coherence score\n",
        "        coherence_score = min(marker_count / len(sentences), 1.0) if sentences else 0\n",
        "\n",
        "        return {\n",
        "            'coherence_score': coherence_score,\n",
        "            'sentence_count': len(sentences),\n",
        "            'avg_sentence_length': avg_sentence_length,\n",
        "            'discourse_markers_found': marker_count,\n",
        "            'readability_indicator': 'Good' if 10 <= avg_sentence_length <= 20 else 'Needs improvement'\n",
        "        }\n",
        "\n",
        "    def _assess_content_preservation(self, original: str, summary: str) -> Dict:\n",
        "        \"\"\"Assess content preservation using multiple metrics\"\"\"\n",
        "        # TF-IDF similarity\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform([original.lower(), summary.lower()])\n",
        "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        except:\n",
        "            similarity = 0.0\n",
        "\n",
        "        # Keyword overlap\n",
        "        orig_tokens = set(preprocessor.tokenize_and_filter(original))\n",
        "        summ_tokens = set(preprocessor.tokenize_and_filter(summary))\n",
        "\n",
        "        if orig_tokens:\n",
        "            overlap_ratio = len(orig_tokens.intersection(summ_tokens)) / len(orig_tokens)\n",
        "        else:\n",
        "            overlap_ratio = 0\n",
        "\n",
        "        return {\n",
        "            'similarity_score': similarity,\n",
        "            'keyword_overlap_ratio': overlap_ratio,\n",
        "            'shared_keywords_count': len(orig_tokens.intersection(summ_tokens)),\n",
        "            'preservation_quality': 'High' if similarity > 0.3 else 'Medium' if similarity > 0.1 else 'Low'\n",
        "        }\n",
        "\n",
        "    def _compare_summaries(self, extractive: str, abstractive: str) -> Dict:\n",
        "        \"\"\"Compare extractive and abstractive summaries\"\"\"\n",
        "        return {\n",
        "            'length_difference': abs(len(extractive.split()) - len(abstractive.split())),\n",
        "            'extractive_length': len(extractive.split()),\n",
        "            'abstractive_length': len(abstractive.split()),\n",
        "            'style_difference': 'Abstractive appears more concise' if len(abstractive.split()) < len(extractive.split()) else 'Similar length'\n",
        "        }\n",
        "\n",
        "    def _calculate_rouge_scores(self, extractive: str, abstractive: str, reference: str) -> Dict:\n",
        "        \"\"\"Calculate ROUGE scores against reference summary\"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        if ROUGE_AVAILABLE:\n",
        "            ext_scores = self.rouge_scorer.score(reference, extractive)\n",
        "            abs_scores = self.rouge_scorer.score(reference, abstractive)\n",
        "\n",
        "            scores = {\n",
        "                'extractive_rouge': {\n",
        "                    'rouge1_f1': ext_scores['rouge1'].fmeasure,\n",
        "                    'rouge2_f1': ext_scores['rouge2'].fmeasure,\n",
        "                    'rougeL_f1': ext_scores['rougeL'].fmeasure\n",
        "                },\n",
        "                'abstractive_rouge': {\n",
        "                    'rouge1_f1': abs_scores['rouge1'].fmeasure,\n",
        "                    'rouge2_f1': abs_scores['rouge2'].fmeasure,\n",
        "                    'rougeL_f1': abs_scores['rougeL'].fmeasure\n",
        "                }\n",
        "            }\n",
        "\n",
        "        return scores\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ComprehensiveEvaluator()\n",
        "print(\"‚úÖ Evaluation system created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LCJiB8TPJTB",
        "outputId": "741e391d-dd10-4917-dd54-a646666c0e86"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =============================================================================\n",
        "# CELL 9: SEARCH AND RETRIEVAL SYSTEM\n",
        "# =============================================================================\n",
        "\n",
        "class DocumentSearcher:\n",
        "    \"\"\"TF-IDF based document search system\"\"\"\n",
        "\n",
        "    def __init__(self, documents_df: pd.DataFrame):\n",
        "        self.documents_df = documents_df\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "        self._build_index()\n",
        "\n",
        "    def _build_index(self):\n",
        "        \"\"\"Build TF-IDF search index\"\"\"\n",
        "        print(\"üîç Building document search index...\")\n",
        "        texts = self.documents_df['processed_text'].fillna('')\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
        "        print(\"‚úÖ Search index ready\")\n",
        "\n",
        "    def search_documents(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search for relevant documents\"\"\"\n",
        "        # Vectorize query\n",
        "        query_vector = self.vectorizer.transform([preprocessor.clean_text(query)])\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
        "\n",
        "        # Get top documents\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] > 0.01:  # Minimum relevance threshold\n",
        "                doc = self.documents_df.iloc[idx]\n",
        "                results.append({\n",
        "                    'headline': doc['headline'],\n",
        "                    'short_description': doc['short_description'],\n",
        "                    'category': doc['category'],\n",
        "                    'date': doc.get('date', 'N/A'),\n",
        "                    'link': doc.get('link', ''),\n",
        "                    'similarity_score': float(similarities[idx]),\n",
        "                    'combined_text': doc['combined_text']\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize document searcher\n",
        "doc_searcher = DocumentSearcher(df_sample)\n",
        "\n",
        "@tool(\n",
        "    name=\"search_news_articles\",\n",
        "    description=\"Search for relevant news articles based on query terms\",\n",
        "    show_result=False,\n",
        "    cache_results=True\n",
        ")\n",
        "def search_articles(query: str, top_k: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Search for relevant articles based on query.\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        top_k (int): Number of articles to return\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of relevant articles with metadata\n",
        "    \"\"\"\n",
        "    return doc_searcher.search_documents(query, top_k)\n",
        "\n",
        "print(\"‚úÖ Document search system ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDV5YJl2PVbW",
        "outputId": "c7beb446-ab1a-4eeb-dd6e-65b38122cabb"
      },
      "outputs": [],
      "source": [
        "\n",
        "@tool(\n",
        "    name=\"comprehensive_news_analysis\",\n",
        "    description=\"Perform comprehensive analysis of news articles on a given topic\",\n",
        "    show_result=True,\n",
        "    cache_results=True,\n",
        "    cache_ttl=1800\n",
        ")\n",
        "def analyze_news_topic(topic: str, num_articles: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Comprehensive analysis of a news topic including entity extraction and summarization.\n",
        "\n",
        "    Args:\n",
        "        topic (str): Topic to analyze\n",
        "        num_articles (int): Number of articles to analyze\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted analysis report\n",
        "    \"\"\"\n",
        "    print(f\"üîç Analyzing topic: '{topic}'\")\n",
        "\n",
        "    # Search for relevant articles\n",
        "    articles = search_articles(topic, num_articles)\n",
        "\n",
        "    if not articles:\n",
        "        return f\"‚ùå No relevant articles found for topic: '{topic}'\"\n",
        "\n",
        "    # Combine article texts\n",
        "    combined_text = ' '.join([article['combined_text'] for article in articles])\n",
        "\n",
        "    # Extract information\n",
        "    entities_ner = extract_entities_ner(combined_text)\n",
        "    entities_regex = extract_entities_regex(combined_text)\n",
        "    keywords = get_top_keywords(combined_text, 8)\n",
        "\n",
        "    # Generate summaries\n",
        "    extractive_summary = extractive_summarize(combined_text, 3)\n",
        "    abstractive_summary = simple_abstractive_summarize(combined_text, 60)\n",
        "\n",
        "    # Perform evaluation\n",
        "    evaluation = evaluator.evaluate_entity_extraction(combined_text, entities_ner, entities_regex)\n",
        "    summary_eval = evaluator.evaluate_summarization(combined_text, extractive_summary, abstractive_summary)\n",
        "\n",
        "    # Format report\n",
        "    report_parts = [\n",
        "        f\"üì∞ NEWS ANALYSIS REPORT: {topic.upper()}\",\n",
        "        \"=\" * 50,\n",
        "        f\"üìä Articles analyzed: {len(articles)}\",\n",
        "        f\"üìÇ Categories: {', '.join(set(article['category'] for article in articles))}\",\n",
        "        \"\",\n",
        "        \"üîç KEY FINDINGS:\",\n",
        "        f\"‚Ä¢ Total entities found: {evaluation['quality_assessment']['total_entities_ner'] + evaluation['quality_assessment']['total_entities_regex']}\",\n",
        "        f\"‚Ä¢ Entity coverage: {evaluation['quality_assessment']['entity_coverage']}\",\n",
        "        f\"‚Ä¢ Content diversity: {evaluation['quality_assessment']['diversity_score']} types\",\n",
        "        \"\",\n",
        "        \"üë• KEY ENTITIES:\",\n",
        "    ]\n",
        "\n",
        "    # Add entity information\n",
        "    if entities_ner:\n",
        "        for entity_type, entities in list(entities_ner.items())[:3]:\n",
        "            if entities:\n",
        "                report_parts.append(f\"‚Ä¢ {entity_type}: {', '.join(entities[:3])}\")\n",
        "\n",
        "    if entities_regex:\n",
        "        for pattern_type, patterns in entities_regex.items():\n",
        "            if patterns:\n",
        "                report_parts.append(f\"‚Ä¢ {pattern_type}: {', '.join(patterns[:2])}\")\n",
        "\n",
        "    report_parts.extend([\n",
        "        \"\",\n",
        "        \"üîë TOP KEYWORDS:\",\n",
        "    ])\n",
        "\n",
        "    for word, score in keywords[:5]:\n",
        "        report_parts.append(f\"‚Ä¢ {word}: {score:.3f}\")\n",
        "\n",
        "    report_parts.extend([\n",
        "        \"\",\n",
        "        \"üìã EXTRACTIVE SUMMARY:\",\n",
        "        f\"{extractive_summary}\",\n",
        "        \"\",\n",
        "        \"üéØ ABSTRACTIVE SUMMARY:\",\n",
        "        f\"{abstractive_summary}\",\n",
        "        \"\",\n",
        "        \"üìä SUMMARY QUALITY METRICS:\",\n",
        "        f\"‚Ä¢ Extractive compression: {summary_eval['extractive_evaluation']['compression']['compression_percentage']:.1f}%\",\n",
        "        f\"‚Ä¢ Abstractive compression: {summary_eval['abstractive_evaluation']['compression']['compression_percentage']:.1f}%\",\n",
        "        f\"‚Ä¢ Content preservation: {summary_eval['extractive_evaluation']['content_preservation']['preservation_quality']}\",\n",
        "        f\"‚Ä¢ Coherence score: {summary_eval['extractive_evaluation']['coherence']['coherence_score']:.2f}\",\n",
        "        \"\",\n",
        "        \"üìë TOP ARTICLES ANALYZED:\",\n",
        "    ])\n",
        "\n",
        "    for i, article in enumerate(articles[:3], 1):\n",
        "        report_parts.append(f\"{i}. {article['headline']} ({article['category']})\")\n",
        "\n",
        "    return '\\n'.join(report_parts)\n",
        "\n",
        "@tool(\n",
        "    name=\"evaluate_nlp_quality\",\n",
        "    description=\"Evaluate the quality of entity extraction and summarization on sample text\",\n",
        "    show_result=True,\n",
        "    requires_confirmation=False\n",
        ")\n",
        "def evaluate_sample_quality(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate NLP processing quality on a given text sample.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to evaluate\n",
        "\n",
        "    Returns:\n",
        "        str: Evaluation report with qualitative examples\n",
        "    \"\"\"\n",
        "    # Extract information\n",
        "    entities_ner = extract_entities_ner(text)\n",
        "    entities_regex = extract_entities_regex(text)\n",
        "\n",
        "    # Generate summaries\n",
        "    extractive_summary = extractive_summarize(text, 2)\n",
        "    abstractive_summary = simple_abstractive_summarize(text, 40)\n",
        "\n",
        "    # Perform evaluations\n",
        "    entity_eval = evaluator.evaluate_entity_extraction(text, entities_ner, entities_regex)\n",
        "    summary_eval = evaluator.evaluate_summarization(text, extractive_summary, abstractive_summary)\n",
        "\n",
        "    # Build evaluation report\n",
        "    report = [\n",
        "        \"üîç NLP QUALITY EVALUATION REPORT\",\n",
        "        \"=\" * 40,\n",
        "        f\"üìù Text length: {len(text.split())} words\",\n",
        "        \"\",\n",
        "        \"üë• ENTITY EXTRACTION EVALUATION:\",\n",
        "        f\"‚Ä¢ NER entities found: {entity_eval['quality_assessment']['total_entities_ner']}\",\n",
        "        f\"‚Ä¢ Regex patterns found: {entity_eval['quality_assessment']['total_entities_regex']}\",\n",
        "        f\"‚Ä¢ Overall coverage: {entity_eval['quality_assessment']['entity_coverage']}\",\n",
        "        \"\",\n",
        "        \"üìã ENTITY EXAMPLES:\",\n",
        "    ]\n",
        "\n",
        "    # Add entity examples\n",
        "    for entity_type, examples in entity_eval['examples']['ner_samples'].items():\n",
        "        if examples:\n",
        "            report.append(f\"‚Ä¢ {entity_type}: {', '.join(examples)}\")\n",
        "\n",
        "    for pattern_type, examples in entity_eval['examples']['regex_samples'].items():\n",
        "        if examples:\n",
        "            report.append(f\"‚Ä¢ {pattern_type}: {', '.join(examples)}\")\n",
        "\n",
        "    report.extend([\n",
        "        \"\",\n",
        "        \"üìä SUMMARIZATION EVALUATION:\",\n",
        "        f\"‚Ä¢ Extractive quality: {summary_eval['extractive_evaluation']['content_preservation']['preservation_quality']}\",\n",
        "        f\"‚Ä¢ Abstractive quality: {summary_eval['abstractive_evaluation']['content_preservation']['preservation_quality']}\",\n",
        "        f\"‚Ä¢ Best approach: {summary_eval['comparative_analysis']['better_content_preservation']}\",\n",
        "        \"\",\n",
        "        \"üìù SUMMARY EXAMPLES:\",\n",
        "        f\"Extractive: {extractive_summary}\",\n",
        "        \"\",\n",
        "        f\"Abstractive: {abstractive_summary}\",\n",
        "        \"\",\n",
        "        \"üìà QUALITY METRICS:\",\n",
        "        f\"‚Ä¢ Content preservation: {summary_eval['extractive_evaluation']['content_preservation']['similarity_score']:.3f}\",\n",
        "        f\"‚Ä¢ Compression ratio: {summary_eval['extractive_evaluation']['compression']['ratio']:.2f}\",\n",
        "        f\"‚Ä¢ Coherence score: {summary_eval['extractive_evaluation']['coherence']['coherence_score']:.2f}\",\n",
        "    ])\n",
        "\n",
        "    return '\\n'.join(report)\n",
        "\n",
        "# Create the News Intelligence Agent\n",
        "news_intelligence_agent = Agent(\n",
        "    name=\"News Intelligence Agent\",\n",
        "    role=\"Advanced news analysis and information extraction specialist\",\n",
        "    instructions=\"\"\"You are a sophisticated news intelligence agent that helps users understand current events and extract insights from news articles.\n",
        "Your capabilities include:\n",
        "- Searching and retrieving relevant news articles\n",
        "- Extracting key entities (people, organizations, locations, dates, etc.)\n",
        "- Generating both extractive and abstractive summaries\n",
        "- Evaluating the quality of information extraction and summarization\n",
        "- Providing comprehensive analysis reports\n",
        "When users ask about news topics, search for relevant articles first, then provide comprehensive analysis including entity extraction, summarization, and quality evaluation. Always include specific examples and metrics in your responses.\"\"\",\n",
        "\n",
        "    tools=[\n",
        "        search_articles,\n",
        "        extract_entities_ner,\n",
        "        extract_entities_regex,\n",
        "        get_top_keywords,\n",
        "        extractive_summarize,\n",
        "        simple_abstractive_summarize,\n",
        "        analyze_news_topic,\n",
        "        evaluate_sample_quality\n",
        "    ],\n",
        "\n",
        "    model=Groq(id=\"llama-3.3-70b-versatile\", api_key=api_key),\n",
        "\n",
        "    show_tool_calls=True,\n",
        "    markdown=True\n",
        ")\n",
        "\n",
        "print(\"ü§ñ News Intelligence Agent created successfully!\")\n",
        "print(\"Available tools:\", [tool.name for tool in news_intelligence_agent.tools])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAei8AZdQEZa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_comprehensive_demo():\n",
        "    \"\"\"Run comprehensive demonstration of the system\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üöÄ NEWS INTELLIGENCE SYSTEM COMPREHENSIVE DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Test topics for demonstration\n",
        "    demo_topics = [\n",
        "        \"artificial intelligence\",\n",
        "        \"climate change\",\n",
        "        \"technology trends\"\n",
        "    ]\n",
        "\n",
        "    # Run analysis on sample topics\n",
        "    for i, topic in enumerate(demo_topics[:2], 1):  # Limit for demo\n",
        "        print(f\"\\nüéØ DEMO {i}: Analyzing '{topic}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        try:\n",
        "            # Use the agent to analyze the topic\n",
        "            response = news_intelligence_agent.run(\n",
        "                f\"Analyze news articles about '{topic}' and provide a comprehensive report including entity extraction, summarization, and quality evaluation.\"\n",
        "            )\n",
        "\n",
        "            print(\"ü§ñ Agent Response:\")\n",
        "            print(response.content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during analysis: {str(e)}\")\n",
        "\n",
        "            # Fallback: Direct tool usage\n",
        "            print(\"üìã Fallback: Using direct tool analysis...\")\n",
        "            result = analyze_news_topic(topic, 3)\n",
        "            print(result)\n",
        "\n",
        "    # Demonstrate quality evaluation on a sample text\n",
        "    print(f\"\\nüîç QUALITY EVALUATION DEMO\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    sample_text = df_sample['combined_text'].iloc[0]\n",
        "    print(\"üìù Evaluating sample text...\")\n",
        "\n",
        "    try:\n",
        "        eval_response = news_intelligence_agent.run(\n",
        "            f\"Evaluate the NLP processing quality on this text: '{sample_text[:200]}...'\"\n",
        "        )\n",
        "        print(\"ü§ñ Evaluation Response:\")\n",
        "        print(eval_response.content)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during evaluation: {str(e)}\")\n",
        "\n",
        "        # Fallback\n",
        "        eval_result = evaluate_sample_quality(sample_text)\n",
        "        print(eval_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxXGXFBDQWf6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def interactive_news_interface():\n",
        "    \"\"\"Interactive interface for testing the news agent\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üí¨ INTERACTIVE NEWS INTELLIGENCE INTERFACE\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Ask me anything about the news! I can:\")\n",
        "    print(\"‚Ä¢ Analyze news topics comprehensively\")\n",
        "    print(\"‚Ä¢ Extract entities and generate summaries\")\n",
        "    print(\"‚Ä¢ Evaluate processing quality\")\n",
        "    print(\"‚Ä¢ Search for specific articles\")\n",
        "    print(\"\\nExample queries:\")\n",
        "    print(\"  - 'Analyze artificial intelligence news'\")\n",
        "    print(\"  - 'What entities can you find in technology articles?'\")\n",
        "    print(\"  - 'Evaluate the quality of sports news summaries'\")\n",
        "    print(\"  - Type 'quit' to exit\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_query = input(\"\\nü§î Your question: \").strip()\n",
        "\n",
        "            if user_query.lower() in ['quit', 'exit', 'bye']:\n",
        "                print(\"üëã Thank you for using the News Intelligence System!\")\n",
        "                break\n",
        "\n",
        "            if not user_query:\n",
        "                continue\n",
        "\n",
        "            print(\"\\nüîç Processing your request...\")\n",
        "\n",
        "            # Use the agent to process the query\n",
        "            try:\n",
        "                response = news_intelligence_agent.run(user_query)\n",
        "                print(f\"\\nü§ñ Response:\\n{response.content}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Agent error: {str(e)}\")\n",
        "                print(\"üí° Tip: Make sure you have set up your  API key or configure a different model\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüëã Session interrupted. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Unexpected error: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs5srpYsQcMU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_detailed_evaluation_examples():\n",
        "    \"\"\"Run detailed evaluation with qualitative examples\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä DETAILED EVALUATION WITH QUALITATIVE EXAMPLES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Select diverse sample texts for evaluation\n",
        "    sample_indices = [0, 10, 25]  # Different categories\n",
        "\n",
        "    for i, idx in enumerate(sample_indices, 1):\n",
        "        if idx >= len(df_sample):\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüîç EVALUATION EXAMPLE {i}\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        sample_row = df_sample.iloc[idx]\n",
        "        text = sample_row['combined_text']\n",
        "\n",
        "        print(f\"üìÇ Category: {sample_row['category']}\")\n",
        "        print(f\"üì∞ Headline: {sample_row['headline']}\")\n",
        "        print(f\"üìù Text preview: {text[:200]}...\")\n",
        "\n",
        "        # Extract entities and evaluate\n",
        "        entities_ner = extract_entities_ner_func(text)\n",
        "        entities_regex = extract_entities_regex_func(text)\n",
        "\n",
        "        print(f\"\\nüë• ENTITIES FOUND:\")\n",
        "        print(f\"NER entities: {sum(len(v) for v in entities_ner.values())}\")\n",
        "        for ent_type, entities in entities_ner.items():\n",
        "            if entities:\n",
        "                print(f\"  ‚Ä¢ {ent_type}: {', '.join(entities[:3])}\")\n",
        "\n",
        "        print(f\"Regex patterns: {sum(len(v) for v in entities_regex.values())}\")\n",
        "        for pattern_type, patterns in entities_regex.items():\n",
        "            if patterns:\n",
        "                print(f\"  ‚Ä¢ {pattern_type}: {', '.join(patterns[:2])}\")\n",
        "\n",
        "        # Generate and evaluate summaries\n",
        "        extractive_summary = extractive_summarize(text, 2)\n",
        "        abstractive_summary = simple_abstractive_summarize(text, 40)\n",
        "\n",
        "        print(f\"\\nüìã SUMMARIES:\")\n",
        "        print(f\"Extractive: {extractive_summary}\")\n",
        "        print(f\"Abstractive: {abstractive_summary}\")\n",
        "\n",
        "        # Quality evaluation\n",
        "        summary_eval = evaluator.evaluate_summarization(text, extractive_summary, abstractive_summary)\n",
        "\n",
        "        print(f\"\\nüìä QUALITY METRICS:\")\n",
        "        print(f\"‚Ä¢ Compression ratio: {summary_eval['extractive_evaluation']['compression']['ratio']:.2f}\")\n",
        "        print(f\"‚Ä¢ Content preservation: {summary_eval['extractive_evaluation']['content_preservation']['similarity_score']:.3f}\")\n",
        "        print(f\"‚Ä¢ Coherence score: {summary_eval['extractive_evaluation']['coherence']['coherence_score']:.3f}\")\n",
        "        print(f\"‚Ä¢ Better approach: {summary_eval['comparative_analysis']['better_content_preservation']}\")\n",
        "\n",
        "        # Qualitative assessment\n",
        "        preservation_quality = summary_eval['extractive_evaluation']['content_preservation']['preservation_quality']\n",
        "        print(f\"‚Ä¢ Overall assessment: {preservation_quality}\")\n",
        "\n",
        "        if ROUGE_AVAILABLE and i == 1:  # Demonstrate ROUGE on first example\n",
        "            print(f\"\\nüìà ROUGE EVALUATION (simulated with extractive as reference):\")\n",
        "            rouge_scores = evaluator._calculate_rouge_scores(abstractive_summary, extractive_summary, extractive_summary)\n",
        "            if rouge_scores:\n",
        "                print(f\"‚Ä¢ ROUGE-1 F1: {rouge_scores.get('abstractive_rouge', {}).get('rouge1_f1', 'N/A'):.3f}\")\n",
        "                print(f\"‚Ä¢ ROUGE-L F1: {rouge_scores.get('abstractive_rouge', {}).get('rougeL_f1', 'N/A'):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9bgpPHBQoFQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def analyze_system_performance():\n",
        "    \"\"\"Analyze overall system performance across multiple samples\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìà SYSTEM PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Sample texts for batch evaluation\n",
        "    sample_size = min(20, len(df_sample))\n",
        "    sample_texts = df_sample['combined_text'].head(sample_size).tolist()\n",
        "\n",
        "    # Initialize metrics\n",
        "    entity_extraction_success = 0\n",
        "    summarization_success = 0\n",
        "    avg_compression_ratios = []\n",
        "    avg_content_preservation = []\n",
        "    category_performance = defaultdict(list)\n",
        "\n",
        "    print(f\"üìä Analyzing {sample_size} articles...\")\n",
        "\n",
        "    for i, text in enumerate(sample_texts):\n",
        "        try:\n",
        "            # Entity extraction\n",
        "            entities_ner = extract_entities_ner(text)\n",
        "            entities_regex = extract_entities_regex(text)\n",
        "\n",
        "            total_entities = sum(len(v) for v in entities_ner.values()) + sum(len(v) for v in entities_regex.values())\n",
        "\n",
        "            if total_entities > 0:\n",
        "                entity_extraction_success += 1\n",
        "\n",
        "            # Summarization\n",
        "            extractive_summary = extractive_summarize(text, 2)\n",
        "\n",
        "            if extractive_summary and len(extractive_summary.split()) > 5:\n",
        "                summarization_success += 1\n",
        "\n",
        "                # Calculate metrics\n",
        "                compression_ratio = len(extractive_summary.split()) / len(text.split())\n",
        "                avg_compression_ratios.append(compression_ratio)\n",
        "\n",
        "                # Content preservation\n",
        "                content_eval = evaluator._assess_content_preservation(text, extractive_summary)\n",
        "                avg_content_preservation.append(content_eval['similarity_score'])\n",
        "\n",
        "                # Category performance\n",
        "                category = df_sample.iloc[i]['category']\n",
        "                category_performance[category].append(content_eval['similarity_score'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing sample {i}: {str(e)}\")\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    entity_success_rate = entity_extraction_success / sample_size\n",
        "    summary_success_rate = summarization_success / sample_size\n",
        "    avg_compression = np.mean(avg_compression_ratios) if avg_compression_ratios else 0\n",
        "    avg_preservation = np.mean(avg_content_preservation) if avg_content_preservation else 0\n",
        "\n",
        "    print(f\"\\nüìä OVERALL PERFORMANCE METRICS:\")\n",
        "    print(f\"‚Ä¢ Entity extraction success rate: {entity_success_rate:.1%}\")\n",
        "    print(f\"‚Ä¢ Summarization success rate: {summary_success_rate:.1%}\")\n",
        "    print(f\"‚Ä¢ Average compression ratio: {avg_compression:.3f}\")\n",
        "    print(f\"‚Ä¢ Average content preservation: {avg_preservation:.3f}\")\n",
        "\n",
        "    print(f\"\\nüìÇ PERFORMANCE BY CATEGORY:\")\n",
        "    for category, scores in category_performance.items():\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            print(f\"‚Ä¢ {category}: {avg_score:.3f} (n={len(scores)})\")\n",
        "\n",
        "    # Performance assessment\n",
        "    print(f\"\\nüéØ SYSTEM ASSESSMENT:\")\n",
        "    if entity_success_rate > 0.8:\n",
        "        print(\"‚úÖ Entity extraction: Excellent performance\")\n",
        "    elif entity_success_rate > 0.6:\n",
        "        print(\"‚úÖ Entity extraction: Good performance\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Entity extraction: Needs improvement\")\n",
        "\n",
        "    if summary_success_rate > 0.8:\n",
        "        print(\"‚úÖ Summarization: Excellent performance\")\n",
        "    elif summary_success_rate > 0.6:\n",
        "        print(\"‚úÖ Summarization: Good performance\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Summarization: Needs improvement\")\n",
        "\n",
        "    return {\n",
        "        'entity_success_rate': entity_success_rate,\n",
        "        'summary_success_rate': summary_success_rate,\n",
        "        'avg_compression': avg_compression,\n",
        "        'avg_preservation': avg_preservation,\n",
        "        'category_performance': dict(category_performance)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Z5tvvWrTRWYV",
        "outputId": "49f39f51-902c-47f2-89e2-657cfea2e5c6"
      },
      "outputs": [],
      "source": [
        "interactive_news_interface()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "collapsed": true,
        "id": "nduGP3yBRmBM",
        "outputId": "34102f55-1303-40ef-c865-686e99133755"
      },
      "outputs": [],
      "source": [
        "run_comprehensive_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcadHqbNR1OK"
      },
      "outputs": [],
      "source": [
        "run_detailed_evaluation_examples()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arNd4u87SBRu"
      },
      "outputs": [],
      "source": [
        "analyze_system_performance()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNZHhio77vCphH5Chcmkxky",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
